{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2Sgfbz+8MfJIJhUeASm+W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgorai005/deeplearning_assignment1/blob/main/dl_assign1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv1dd-oHSJQw"
      },
      "outputs": [],
      "source": [
        "# Name-Biswajit gorai ,Roll no- MA24M005"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "import wandb\n",
        "\n",
        "# Login to Weights & Biases\n",
        "wandb.login()\n",
        "\n",
        "# Initialize W&B project\n",
        "wandb.init(project='deep_learning_assignment_1')\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Define the class labels\n",
        "class_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Plot 1 sample image for each class\n",
        "plt.figure(figsize=(10, 10))\n",
        "for label in range(10):\n",
        "    sample_image = train_images[train_labels == label][0]\n",
        "    plt.subplot(5, 5, label + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.xlabel(class_labels[label])\n",
        "    plt.imshow(sample_image, cmap=plt.cm.binary)\n",
        "plt.show()\n",
        "\n",
        "# Finish the W&B run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "odyBhgZjeOAS",
        "outputId": "80248246-aec2-44dc-9725-0f1e5c32044b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250306_040715-khz9402p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/deep_learning_assignment_1/runs/khz9402p' target=\"_blank\">sage-snow-8</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/deep_learning_assignment_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bgorai005-iit-madras/deep_learning_assignment_1' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/deep_learning_assignment_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bgorai005-iit-madras/deep_learning_assignment_1/runs/khz9402p' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/deep_learning_assignment_1/runs/khz9402p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sage-snow-8</strong> at: <a href='https://wandb.ai/bgorai005-iit-madras/deep_learning_assignment_1/runs/khz9402p' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/deep_learning_assignment_1/runs/khz9402p</a><br> View project at: <a href='https://wandb.ai/bgorai005-iit-madras/deep_learning_assignment_1' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/deep_learning_assignment_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250306_040715-khz9402p/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_valid,y_train,y_valid = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "tgYZpymnY750"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#flattenten the image data\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_valid = x_valid.reshape(x_valid.shape[0], -1)\n",
        "x_test = test_images.reshape(test_images.shape[0], -1)"
      ],
      "metadata": {
        "id": "C9Ihs9sfa0CF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize the image data\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_valid = x_valid.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n"
      ],
      "metadata": {
        "id": "CQfNOgItbJxb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding for output\n",
        "def one_hot_encode(labels):\n",
        "    encoded = np.zeros((len(labels), 10))\n",
        "    for i, label in enumerate(labels):\n",
        "        encoded[i, label] = 1\n",
        "    return encoded\n",
        "\n",
        "y_train = one_hot_encode(y_train)\n",
        "y_valid = one_hot_encode(y_valid)\n",
        "y_test = one_hot_encode(test_labels)"
      ],
      "metadata": {
        "id": "_Pz9uaumjx58"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shapes of the datasets\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_valid shape:\", x_valid.shape)\n",
        "print(\"y_valid shape:\", y_valid.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LApDeYDeZGuv",
        "outputId": "b3300ee7-f706-49c5-c386-43b9e06fab71"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (54000, 784)\n",
            "y_train shape: (54000, 10)\n",
            "x_valid shape: (6000, 784)\n",
            "y_valid shape: (6000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# create a feedforward neural network\n",
        "class FeedForwardNeuralNetwork:\n",
        "  def __init__(self,input_size,num_layers=1,hidden_size=4,output_size=1,weight_int='random'):\n",
        "    self.input_size=input_size\n",
        "    self.num_layers=num_layers\n",
        "    self.hidden_size=hidden_size\n",
        "    self.output_size=output_size\n",
        "    self.weight_int = weight_int\n",
        "    self.weights = self.initialize_weights()\n",
        "\n",
        "  def initialize_weights(self):\n",
        "    network_size=[]\n",
        "    for i in range(self.num_layers):\n",
        "      network_size.append(self.hidden_size)\n",
        "    network_size=[self.input_size]+network_size+[self.output_size]\n",
        "    weights={}\n",
        "    for i in range(1,self.num_layers+2):\n",
        "     if self.weight_int=='random':\n",
        "      weights[f'w{i}']=np.random.randn(network_size[i],network_size[i-1])\n",
        "      weights[f'b{i}']=np.random.randn(network_size[i],1)\n",
        "     elif self.weight_int=='xavier':\n",
        "      weights[f'w{i}']=np.random.rand(network_size[i],network_size[i-1])*np.sqrt(2/network_size[i-1])\n",
        "      weights[f'b{i}']=np.random.rand(network_size[i],1)*np.sqrt( 2/network_size[i-1])\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "  def sigmoid(self,x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "\n",
        "  def softmax(self,x):\n",
        "    x=x-np.max(x)\n",
        "    return np.exp(x)/np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "  def forward_prop(self,x):\n",
        "    a={}\n",
        "    h={'h0':x}\n",
        "\n",
        "    for i in range(1,self.hidden_size+1):\n",
        "      a[f'a{i}']=np.dot(self.weights[f'w{i}'],h[f'h{i-1}'])+self.weights[f'b{i}']\n",
        "      h[f'h{i}']=self.sigmoid(a[f'a{i}'])\n",
        "    a[f'a{self.hidden_size+1}']=np.dot(self.weights[f'w{self.hidden_size+1}',h[f'h{self.hidden_size}']])  +self.weights[f'b{self.hidden_size+1}']\n",
        "    y_pred=self.softmax(a[f'a{self.hidden_size+1}'])\n",
        "    return a,h,y_pred\n",
        "\n",
        "  def back_prop(self,x,y_actual,theta,input_size):\n",
        "    m=x.shape[0]\n",
        "    a,h,y_pred=self.forward_prop(x,theta,self.num_layers,self.input_size)\n",
        "    grad_theta={}\n",
        "    grad_h_a={}\n",
        "    grad_h_a[f'a{self.num_layers+1}']=-1*(y_actual-y_pred)\n",
        "    for i in range(self.num_layers+1,0,-1):\n",
        "      grad_theta[f'w{i}']=np.dot(grad_h_a[f'a{i}'],h[f'h{i-1}'].T)*(1/m)\n",
        "      grad_theta[f'b{i}']=np.mean(grad_h_a[f'a{i}'],axis=1,keepdims=True)*(1/m)\n",
        "      grad_h_a[f'h{i-1}']=np.dot(self.weights[f'w{i}'].T,grad_h_a[f'a{i}'])\n",
        "      grad_h_a[f'a{i-1}']=grad_h_a[f'h{i-1}']*(self.der_sigmoid(h[f'h{i-1}']))\n",
        "    return grad_theta\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "firstnetwork=FeedForwardNeuralNetwork(input_size=784,num_layers=3,hidden_size=32,output_size=10,weight_int='random')\n",
        "print(firstnetwork.weights.keys())\n"
      ],
      "metadata": {
        "id": "ml7HxlgEUkxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782b92fc-7a10-4199-9c90-fe10c26b05ba"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['w1', 'b1', 'w2', 'b2', 'w3', 'b3', 'w4', 'b4'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26Fx-hMxavrT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intialize a network"
      ],
      "metadata": {
        "id": "-LNrA7j7Z1fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_weight(num_layer, hidden_size, weight_ini_method):\n",
        "  network_size=[]\n",
        "  for i in range(num_layer):\n",
        "    network_size.append(hidden_size[i])\n",
        "  network_size=[x_train.shape[1]]+network_size+[y_train.shape[1]]\n",
        "  print(network_size)\n",
        "  theta={}\n",
        "  if weight_ini_method==\"random\":\n",
        "    for i in range(len(network_size)-1):\n",
        "      theta[f'W{i+1}']=np.random.randn(network_size[i],network_size[i+1])\n",
        "      theta[f'b{i+1}']=np.random.randn(1,network_size[i+1])\n",
        "  if weight_ini_method==\"Xavier\":\n",
        "    for i in range(len(network_size)-1):\n",
        "      theta[f'W{i+1}']=np.random.randn(network_size[i],network_size[i+1])*(np.sqrt(2/(network_size[i])))\n",
        "      theta[f'b{i+1}']=np.random.randn(1,network_size[i+1])*(np.sqrt(2/(network_size[i])))\n",
        "  return theta\n"
      ],
      "metadata": {
        "id": "iCiSQYSkVaiv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Activation functiont**"
      ],
      "metadata": {
        "id": "xl3fOo4maq91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1 /(1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "\treturn np.tanh(x)\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def deriv_relu(x):\n",
        "  return np.where(x <= 0, 0, 1)\n",
        "\n",
        "def deriv_tanh(x):\n",
        "  return (1 - (tanh(x))**2)\n",
        "\n",
        "def softmax(x):\n",
        "  x = x - np.max(x,axis=1,keepdims=True)\n",
        "  return np.exp(x)/np.sum(np.exp(x),axis=1,keepdims=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def activation(x, activ_fun):\n",
        "  if activ_fun == 'sigmoid':\n",
        "    return sigmoid(x)\n",
        "  elif activ_fun == 'relu':\n",
        "    return relu(x)\n",
        "  elif activ_fun=='softmax':\n",
        "    return softmax(x)\n",
        "  else:\n",
        "    return tanh(x)\n",
        "\n",
        "def deriv_activation(x, activ_fun):\n",
        "  if activ_fun == 'sigmoid':\n",
        "    return deriv_sigmoid(x)\n",
        "  elif activ_fun == 'relu':\n",
        "    return deriv_relu(x)\n",
        "  else:\n",
        "    return deriv_tanh(x)\n"
      ],
      "metadata": {
        "id": "r4rfuQJNamLG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function"
      ],
      "metadata": {
        "id": "9ISyJWybbDbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(y_actual, y_pred):\n",
        "   epsilon = 1e-15  # to prevent log(0) which is undefined\n",
        "   y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "   loss = -np.mean(np.sum(y_actual * np.log(y_pred), axis=1))\n",
        "   return loss\n"
      ],
      "metadata": {
        "id": "PmmO0ZsGbCtI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(X, theta, active_fun_hidden, active_fun_output, num_layers):\n",
        "\n",
        "  a = {}\n",
        "  h = {'h0': X}\n",
        "  for i in range(1, num_layers + 1):\n",
        "    a[f'a{i}'] = np.dot(h[f'h{i-1}'], theta[f'W{i}']) + theta[f'b{i}']\n",
        "    h[f'h{i}'] = activation(a[f'a{i}'], active_fun_hidden)\n",
        "  a[f'a{num_layers+1}'] = np.dot(h[f'h{num_layers}'], theta[f'W{num_layers+1}']) + theta[f'b{num_layers+1}']\n",
        "  y_pred = activation(a[f'a{num_layers+1}'], active_fun_output)\n",
        "  return a, h, y_pred\n"
      ],
      "metadata": {
        "id": "HEQ6ssVcbKyx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta=initial_weight(5,[5]*5,\"random\")\n",
        "_,_,y_pred=forward_prop(x_train,theta,\"sigmoid\",'softmax',5)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjRZ1McJbfgL",
        "outputId": "c53a7049-a647-4bcd-9131-755fbb60d68a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[784, 5, 5, 5, 5, 5, 10]\n",
            "[[0.00264285 0.10974322 0.02964158 ... 0.05911816 0.24916442 0.00368829]\n",
            " [0.00262616 0.11320528 0.02992949 ... 0.06028976 0.24736121 0.00373784]\n",
            " [0.00265595 0.10909447 0.02970422 ... 0.05911877 0.24935161 0.00367943]\n",
            " ...\n",
            " [0.00267445 0.1095117  0.0299913  ... 0.05970226 0.24896153 0.00368706]\n",
            " [0.00265792 0.10808301 0.02965059 ... 0.05888335 0.2493473  0.00366405]\n",
            " [0.00261201 0.11559204 0.03001332 ... 0.06089415 0.24643116 0.00377088]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rVjHCvqqbgVH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate accuracy"
      ],
      "metadata": {
        "id": "MTgaZ5o2cBaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_accuracy(x,y,theta,active_fun_hidden,active_fun_output,num_layers):\n",
        "    # Get the input size from X (number of features)\n",
        "    input_size = x.shape[0]\n",
        "\n",
        "    # Perform forward propagation to get predicted probabilities\n",
        "    # We only need y_pred, so ignore other returned values with _\n",
        "    _, _, y_pred = forward_prop(x, theta,\n",
        "                               active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "    # Get true class labels from one-hot encoded Y (index of 1 in each column)\n",
        "    true_classes = np.argmax(y, axis=1)\n",
        "\n",
        "    # Get predicted class labels from y_pred (index of max probability in each column)\n",
        "    predicted_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Calculate accuracy as the proportion of correct predictions\n",
        "    accuracy = np.mean(predicted_classes == true_classes)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "d_YCl96OcCmm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4Jghwh3cEK4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back_prop"
      ],
      "metadata": {
        "id": "HtmOP1MBcIkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(x,y,theta,active_fun_hidden,active_fun_output,num_layers):\n",
        "  m=x.shape[0]\n",
        "  a,h,y_pred=forward_prop(x,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "  grad_theta={}\n",
        "  grad_h_a={}\n",
        "  grad_h_a[f'a{num_layers+1}']=-1*(y-y_pred)\n",
        "  for i in range(num_layers+1,0,-1):\n",
        "    grad_theta[f'W{i}']=np.dot(h[f'h{i-1}'].T,grad_h_a[f'a{i}'])\n",
        "    grad_theta[f'b{i}']=np.sum(grad_h_a[f'a{i}'],axis=0,keepdims=True)\n",
        "    if i>1:\n",
        "      grad_h_a[f'h{i-1}']=np.dot(grad_h_a[f'a{i}'],theta[f'W{i}'].T)\n",
        "      grad_h_a[f'a{i-1}']=grad_h_a[f'h{i-1}']*deriv_activation(a[f'a{i-1}'],active_fun_hidden)\n",
        "  return grad_theta"
      ],
      "metadata": {
        "id": "02PbU5yicGuj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we again write gradient decesent algorithm compatiable with batch size ,"
      ],
      "metadata": {
        "id": "zMsZdYfvcU78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(lr, x_train, y_train, x_val, y_val, batch_size, num_layers, hidden_size, active_fun_hidden, active_fun_output, weight_ini_method, weight_decay, epochs):\n",
        "    # Initialize weights\n",
        "    theta = initial_weight(num_layers, hidden_size, weight_ini_method)\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle training data\n",
        "        indices = np.arange(x_train.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        x_train = x_train[indices]\n",
        "        y_train = y_train[indices]\n",
        "\n",
        "        # Mini-batch training\n",
        "        for i in range(0, x_train.shape[0], batch_size):\n",
        "            x_batch = x_train[i:i + batch_size]\n",
        "            y_batch = y_train[i:i + batch_size]\n",
        "\n",
        "            # Compute gradients and update weights\n",
        "            grad = back_prop(x_batch, y_batch, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "            for j in range(1, num_layers + 2):\n",
        "                theta[f'W{j}'] -= lr * (grad[f'W{j}'] + weight_decay * theta[f'W{j}'])\n",
        "                theta[f'b{j}'] -= lr * grad[f'b{j}']\n",
        "        #compute the train accuracy\n",
        "        train_accuracy=cal_accuracy(x_train,y_train,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "        # Compute and store training loss\n",
        "        _, _, y_train_pred = forward_prop(x_train, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "        train_loss = cross_entropy(y_train, y_train_pred)\n",
        "        train_loss_history.append(train_loss)\n",
        "\n",
        "        #compute the validation accuracy\n",
        "        validation_accuracy=cal_accuracy(x_val,y_val,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "        # Compute and store validation loss\n",
        "        _, _, y_val_pred = forward_prop(x_val, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "        val_loss = cross_entropy(y_val, y_val_pred)\n",
        "        val_loss_history.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
        "        wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss,\"train_accuracy\":train_accuracy*100,\"val_accuracy\":validation_accuracy*100})\n",
        "\n",
        "    # Final test accuracy\n",
        "    test_accuracy = cal_accuracy(x_test, y_test, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "    print(f'Test Accuracy: {test_accuracy*100}')\n",
        "    wandb.log({\"test_accuracy\": test_accuracy*100})\n",
        "\n",
        "    # # Plot training and validation loss\n",
        "    # plt.plot(train_loss_history, label='Training Loss')\n",
        "    # plt.plot(val_loss_history, label='Validation Loss')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Loss')\n",
        "    # plt.title('Training and Validation Loss over Epochs')\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "    return theta, train_loss_history, val_loss_history\n"
      ],
      "metadata": {
        "id": "3avsjea6cNu0"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moment Gradient descent (here momentum=beta)"
      ],
      "metadata": {
        "id": "k9o-RzjFry14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mgd(lr, momentum, x_train, y_train, x_val, y_val, batch_size, num_layers, hidden_size, active_fun_hidden, active_fun_output, weight_ini_method, weight_decay, epochs):\n",
        "    theta = initial_weight(num_layers, hidden_size, weight_ini_method)\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    # Initialize velocity terms for weights and biases\n",
        "    velocity = {}\n",
        "    for i in range(1, num_layers + 2):\n",
        "        velocity[f'W{i}'] = np.zeros_like(theta[f'W{i}'])\n",
        "        velocity[f'b{i}'] = np.zeros_like(theta[f'b{i}'])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle training data\n",
        "        indices = np.arange(x_train.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        x_train = x_train[indices]\n",
        "        y_train = y_train[indices]\n",
        "\n",
        "        # Mini-batch training\n",
        "        for i in range(0, x_train.shape[0], batch_size):\n",
        "            x_batch = x_train[i:i + batch_size]\n",
        "            y_batch = y_train[i:i + batch_size]\n",
        "\n",
        "            # Compute gradients\n",
        "            grad = back_prop(x_batch, y_batch, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "\n",
        "            # Update weights and biases using momentum\n",
        "            for j in range(1, num_layers + 2):\n",
        "                velocity[f'W{j}'] = momentum * velocity[f'W{j}'] + grad[f'W{j}']\n",
        "                velocity[f'b{j}'] = momentum * velocity[f'b{j}'] +grad[f'b{j}']\n",
        "\n",
        "                theta[f'W{j}'] -= lr*(velocity[f'W{j}'] + weight_decay * theta[f'W{j}'])\n",
        "                theta[f'b{j}'] -= lr*velocity[f'b{j}']\n",
        "        #compute the train accuracy\n",
        "        train_accuracy=cal_accuracy(x_train,y_train,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "\n",
        "        # Compute and store training loss\n",
        "        _, _, y_train_pred = forward_prop(x_train, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "        train_loss = cross_entropy(y_train, y_train_pred)\n",
        "        train_loss_history.append(train_loss)\n",
        "\n",
        "        #compute the validation accuracy\n",
        "        validation_accuracy=cal_accuracy(x_val,y_val,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "        # Compute and store validation loss\n",
        "        _, _, y_val_pred = forward_prop(x_val, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "        val_loss = cross_entropy(y_val, y_val_pred)\n",
        "        val_loss_history.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
        "        wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss,\"train_accuracy\":train_accuracy*100,\"val_accuracy\":validation_accuracy*100})\n",
        "\n",
        "\n",
        "    # Calculate final accuracy\n",
        "    accuracy = cal_accuracy(x_test, y_test, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "    print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "    # # Plot training and validation loss\n",
        "    # plt.plot(train_loss_history, label='Training Loss')\n",
        "    # plt.plot(val_loss_history, label='Validation Loss')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Loss')\n",
        "    # plt.title('Training and Validation Loss over Epochs')\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "    return theta, train_loss_history, val_loss_history\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "6rZVfVJucfGn",
        "outputId": "c7f76d40-a4b3-498c-ce08-15ea010aa8b6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[784, 10, 20, 10, 10, 19, 10]\n",
            "Epoch 1, Train Loss: 1.897368039582914, Val Loss: 1.8972212023318913\n",
            "Epoch 2, Train Loss: 1.5457180149827698, Val Loss: 1.5564540620478473\n",
            "Epoch 3, Train Loss: 1.3559394457819056, Val Loss: 1.3706157859682753\n",
            "Epoch 4, Train Loss: 1.244175385634155, Val Loss: 1.2659661444215762\n",
            "Epoch 5, Train Loss: 1.1749150999362754, Val Loss: 1.1930806452592464\n",
            "Epoch 6, Train Loss: 1.1203733896052859, Val Loss: 1.1385653937307938\n",
            "Epoch 7, Train Loss: 1.070198073339923, Val Loss: 1.0843670651447648\n",
            "Epoch 8, Train Loss: 1.024649471254292, Val Loss: 1.0401750663006948\n",
            "Epoch 9, Train Loss: 0.9928806808607911, Val Loss: 1.0074184732155684\n",
            "Epoch 10, Train Loss: 0.9603229249276273, Val Loss: 0.9758556016207874\n",
            "Test Accuracy: 60.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nesterov accelerated gradient descent**"
      ],
      "metadata": {
        "id": "vcfR044tvXhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def nesterov_gd(lr, momentum, x, y, x_val, y_val, batch_size, num_layers, hidden_size, active_fun_hidden, active_fun_output, weight_ini_method, epochs):\n",
        "    # Initialize network parameters\n",
        "    theta = initial_weight(num_layers, hidden_size, weight_ini_method)\n",
        "\n",
        "    # Initialize velocity dictionary with zeros, matching the shape of each parameter\n",
        "    velocity = {key: np.zeros_like(theta[key]) for key in theta.keys()}\n",
        "\n",
        "    # List to store loss history\n",
        "    loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle training data\n",
        "        indices = np.arange(x.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        x = x[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "        for i in range(0, x.shape[0], batch_size):\n",
        "            x_batch = x[i:i + batch_size]\n",
        "            y_batch = y[i:i + batch_size]\n",
        "\n",
        "            # Look ahead step for Nesterov momentum\n",
        "            lookahead_theta = {key: theta[key] - momentum * velocity[key] for key in theta.keys()}\n",
        "\n",
        "            # Compute gradients using backpropagation on the lookahead parameters\n",
        "            grad = back_prop(x_batch, y_batch, lookahead_theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "\n",
        "            # Update velocities and parameters with Nesterov momentum\n",
        "            for k in range(1, num_layers + 2):\n",
        "                # Update velocity for weights\n",
        "                velocity[f'W{k}'] = momentum * velocity[f'W{k}'] + grad[f'W{k}']\n",
        "                # Update weights using velocity\n",
        "                theta[f'W{k}'] -= lr * velocity[f'W{k}']\n",
        "\n",
        "                # Update velocity for biases\n",
        "                velocity[f'b{k}'] = momentum * velocity[f'b{k}'] + grad[f'b{k}']\n",
        "                # Update biases using velocity\n",
        "                theta[f'b{k}'] -= lr * velocity[f'b{k}']\n",
        "\n",
        "        #compute the train accuracy\n",
        "        train_accuracy=cal_accuracy(x_train,y_train,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "        # Compute training loss and predictions\n",
        "        _, _, y_pred = forward_prop(x, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "        train_loss = cross_entropy(y, y_pred)\n",
        "        loss_history.append(train_loss)\n",
        "\n",
        "        #compute the validation accuracy\n",
        "        validation_accuracy=cal_accuracy(x_val,y_val,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "\n",
        "        # Compute validation loss\n",
        "        _, _, y_val_pred = forward_prop(x_val, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "        val_loss = cross_entropy(y_val, y_val_pred)\n",
        "        val_loss_history.append(val_loss)\n",
        "\n",
        "        # Print loss for monitoring\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
        "        wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss,\"train_accuracy\":train_accuracy*100,\"val_accuracy\":validation_accuracy*100})\n",
        "\n",
        "    # Calculate final accuracy\n",
        "    accuracy = cal_accuracy(x_test, y_test, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "    print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "    # Plot the training and validation loss\n",
        "    # plt.plot(loss_history, label='Training Loss')\n",
        "    # plt.plot(val_loss_history, label='Validation Loss')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Loss')\n",
        "    # plt.title('Training and Validation Loss over Epochs')\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "    return theta, loss_history, val_loss_history\n"
      ],
      "metadata": {
        "id": "NqrxSWryoy54"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RMS Prop**"
      ],
      "metadata": {
        "id": "BbTFuJZhwaI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(lr, gamma, epsilon, x, y, x_val, y_val, batch_size, num_layers, hidden_size, active_fun_hidden, active_fun_output, weight_ini_method, epochs):\n",
        "    # Initialize network parameters\n",
        "    theta = initial_weight(num_layers, hidden_size, weight_ini_method)\n",
        "\n",
        "    # Initialize squared gradient dictionary with zeros, matching the shape of each parameter\n",
        "    squared_grad = {key: np.zeros_like(theta[key]) for key in theta.keys()}\n",
        "\n",
        "    # Lists to store training and validation loss history\n",
        "    loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    # Training loop over epochs\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle training data\n",
        "        indices = np.arange(x.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        x = x[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "        for i in range(0, x.shape[0], batch_size):\n",
        "            x_batch = x[i:i + batch_size]\n",
        "            y_batch = y[i:i + batch_size]\n",
        "\n",
        "            # Compute gradients using backpropagation\n",
        "            grad = back_prop(x_batch, y_batch, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "\n",
        "            # Update squared gradients and parameters with RMSprop\n",
        "            for k in range(1, num_layers + 2):\n",
        "                # Update squared gradient for weights\n",
        "                squared_grad[f'W{k}'] = gamma * squared_grad[f'W{k}'] + (1 - gamma) * (grad[f'W{k}'] ** 2)\n",
        "                # Update weights using RMSprop\n",
        "                theta[f'W{k}'] -= lr * grad[f'W{k}'] / (np.sqrt(squared_grad[f'W{k}']) + epsilon)\n",
        "\n",
        "                # Update squared gradient for biases\n",
        "                squared_grad[f'b{k}'] = gamma * squared_grad[f'b{k}'] + (1 - gamma) * (grad[f'b{k}'] ** 2)\n",
        "                # Update biases using RMSprop\n",
        "                theta[f'b{k}'] -= lr * grad[f'b{k}'] / (np.sqrt(squared_grad[f'b{k}']) + epsilon)\n",
        "\n",
        "       #compute the train accuracy\n",
        "        train_accuracy=cal_accuracy(x_train,y_train,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "        # Compute training loss and predictions\n",
        "        _, _, y_pred = forward_prop(x, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "        train_loss = cross_entropy(y, y_pred)\n",
        "        loss_history.append(train_loss)\n",
        "\n",
        "        #compute the validation accuracy\n",
        "        validation_accuracy=cal_accuracy(x_val,y_val,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "\n",
        "        # Compute validation loss\n",
        "        _, _, y_val_pred = forward_prop(x_val, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "        val_loss = cross_entropy(y_val, y_val_pred)\n",
        "        val_loss_history.append(val_loss)\n",
        "\n",
        "        # Print loss for monitoring\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
        "        wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss,\"train_accuracy\":train_accuracy*100,\"val_accuracy\":validation_accuracy*100})\n",
        "\n",
        "    # Calculate final accuracy on test set\n",
        "    accuracy = cal_accuracy(x_test, y_test, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "    print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    # plt.plot(loss_history, label='Training Loss')\n",
        "    # plt.plot(val_loss_history, label='Validation Loss')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Loss')\n",
        "    # plt.title('Training and Validation Loss over Epochs')\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "    return theta\n"
      ],
      "metadata": {
        "id": "SdNS_n2owQh1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ADAM**"
      ],
      "metadata": {
        "id": "BHWZl5YvwyZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(lr, beta1, beta2, epsilon, x, y, x_val, y_val, batch_size, num_layers, hidden_size, active_fun_hidden, active_fun_output, weight_ini_method, epochs):\n",
        "    # Initialize network parameters\n",
        "    theta = initial_weight(num_layers, hidden_size, weight_ini_method)\n",
        "\n",
        "    # Initialize first moment (m) and second moment (v) dictionaries with zeros\n",
        "    m = {key: np.zeros_like(theta[key]) for key in theta.keys()}\n",
        "    v = {key: np.zeros_like(theta[key]) for key in theta.keys()}\n",
        "\n",
        "    # Lists to store loss history\n",
        "    loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    # Training loop over epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Shuffle training data\n",
        "        indices = np.arange(x.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        x = x[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "        for i in range(0, x.shape[0], batch_size):\n",
        "            x_batch = x[i:i + batch_size]\n",
        "            y_batch = y[i:i + batch_size]\n",
        "\n",
        "            # Compute gradients using backpropagation\n",
        "            grad = back_prop(x_batch, y_batch, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "\n",
        "            # Update first and second moment estimates and parameters with Adam\n",
        "            for k in range(1, num_layers + 2):\n",
        "                # Update biased first moment estimate\n",
        "                m[f'W{k}'] = beta1 * m[f'W{k}'] + (1 - beta1) * grad[f'W{k}']\n",
        "                m[f'b{k}'] = beta1 * m[f'b{k}'] + (1 - beta1) * grad[f'b{k}']\n",
        "\n",
        "                # Update biased second moment estimate\n",
        "                v[f'W{k}'] = beta2 * v[f'W{k}'] + (1 - beta2) * (grad[f'W{k}'] ** 2)\n",
        "                v[f'b{k}'] = beta2 * v[f'b{k}'] + (1 - beta2) * (grad[f'b{k}'] ** 2)\n",
        "\n",
        "                # Compute bias-corrected first and second moment estimates\n",
        "                m_hat_W = m[f'W{k}'] / (1 - beta1 ** epoch)\n",
        "                m_hat_b = m[f'b{k}'] / (1 - beta1 ** epoch)\n",
        "                v_hat_W = v[f'W{k}'] / (1 - beta2 ** epoch)\n",
        "                v_hat_b = v[f'b{k}'] / (1 - beta2 ** epoch)\n",
        "\n",
        "                # Update weights and biases\n",
        "                theta[f'W{k}'] -= lr * m_hat_W / (np.sqrt(v_hat_W) + epsilon)\n",
        "                theta[f'b{k}'] -= lr * m_hat_b / (np.sqrt(v_hat_b) + epsilon)\n",
        "\n",
        "        #compute the train accuracy\n",
        "        train_accuracy=cal_accuracy(x_train,y_train,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "        # Compute training loss and predictions\n",
        "        _, _, y_pred = forward_prop(x, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "        train_loss = cross_entropy(y, y_pred)\n",
        "        loss_history.append(train_loss)\n",
        "\n",
        "        #compute the validation accuracy\n",
        "        validation_accuracy=cal_accuracy(x_val,y_val,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "\n",
        "        # Compute validation loss\n",
        "        _, _, y_val_pred = forward_prop(x_val, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "        val_loss = cross_entropy(y_val, y_val_pred)\n",
        "        val_loss_history.append(val_loss)\n",
        "\n",
        "        # Print loss for monitoring\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
        "        wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss,\"train_accuracy\":train_accuracy*100,\"val_accuracy\":validation_accuracy*100})\n",
        "\n",
        "    # Calculate final accuracy\n",
        "    accuracy = cal_accuracy(x_test, y_test, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "    print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "    # Plot the training and validation loss\n",
        "    # plt.plot(loss_history, label='Training Loss')\n",
        "    # plt.plot(val_loss_history, label='Validation Loss')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Loss')\n",
        "    # plt.title('Training and Validation Loss over Epochs')\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "    return theta\n"
      ],
      "metadata": {
        "id": "-Spq_7iOw1Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NDAM**"
      ],
      "metadata": {
        "id": "eUd42_MyxMur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def nadam(lr, beta1, beta2, epsilon, x, y, x_val, y_val, batch_size, num_layers, hidden_size, active_fun_hidden, active_fun_output, weight_ini_method, epochs):\n",
        "    # Initialize network parameters\n",
        "    theta = initial_weight(num_layers, hidden_size, weight_ini_method)\n",
        "\n",
        "    # Initialize first moment (m) and second moment (v) dictionaries with zeros\n",
        "    m = {key: np.zeros_like(theta[key]) for key in theta.keys()}\n",
        "    v = {key: np.zeros_like(theta[key]) for key in theta.keys()}\n",
        "\n",
        "    # Lists to store loss history\n",
        "    loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    # Training loop over epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Shuffle training data\n",
        "        indices = np.arange(x.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        x = x[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "        for i in range(0, x.shape[0], batch_size):\n",
        "            x_batch = x[i:i + batch_size]\n",
        "            y_batch = y[i:i + batch_size]\n",
        "\n",
        "            # Compute gradients using backpropagation\n",
        "            grad = back_prop(x_batch, y_batch, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "\n",
        "            # Update first and second moment estimates and parameters with Nadam\n",
        "            for k in range(1, num_layers + 2):\n",
        "                # Update biased first moment estimate\n",
        "                m[f'W{k}'] = beta1 * m[f'W{k}'] + (1 - beta1) * grad[f'W{k}']\n",
        "                m[f'b{k}'] = beta1 * m[f'b{k}'] + (1 - beta1) * grad[f'b{k}']\n",
        "\n",
        "                # Update biased second moment estimate\n",
        "                v[f'W{k}'] = beta2 * v[f'W{k}'] + (1 - beta2) * (grad[f'W{k}'] ** 2)\n",
        "                v[f'b{k}'] = beta2 * v[f'b{k}'] + (1 - beta2) * (grad[f'b{k}'] ** 2)\n",
        "\n",
        "                # Compute bias-corrected first and second moment estimates with Nesterov acceleration\n",
        "                m_hat_W = (beta1 * m[f'W{k}'] + (1 - beta1) * grad[f'W{k}']) / (1 - beta1 ** epoch)\n",
        "                m_hat_b = (beta1 * m[f'b{k}'] + (1 - beta1) * grad[f'b{k}']) / (1 - beta1 ** epoch)\n",
        "                v_hat_W = v[f'W{k}'] / (1 - beta2 ** epoch)\n",
        "                v_hat_b = v[f'b{k}'] / (1 - beta2 ** epoch)\n",
        "\n",
        "                # Update weights and biases\n",
        "                theta[f'W{k}'] -= lr * m_hat_W / (np.sqrt(v_hat_W) + epsilon)\n",
        "                theta[f'b{k}'] -= lr * m_hat_b / (np.sqrt(v_hat_b) + epsilon)\n",
        "\n",
        "         #compute the train accuracy\n",
        "        train_accuracy=cal_accuracy(x_train,y_train,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "        # Compute training loss and predictions\n",
        "        _, _, y_pred = forward_prop(x, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "        train_loss = cross_entropy(y, y_pred)\n",
        "        loss_history.append(train_loss)\n",
        "\n",
        "        #compute the validation accuracy\n",
        "        validation_accuracy=cal_accuracy(x_val,y_val,theta,active_fun_hidden,active_fun_output,num_layers)\n",
        "\n",
        "\n",
        "        # Compute validation loss\n",
        "        _, _, y_val_pred = forward_prop(x_val, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "        val_loss = cross_entropy(y_val, y_val_pred)\n",
        "        val_loss_history.append(val_loss)\n",
        "\n",
        "        # Print loss for monitoring\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
        "        wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss,\"train_accuracy\":train_accuracy*100,\"val_accuracy\":validation_accuracy*100})\n",
        "\n",
        "    # Calculate final accuracy\n",
        "    accuracy = cal_accuracy(x_test, y_test, theta, active_fun_hidden, active_fun_output, num_layers)\n",
        "    print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "    # Plot the training and validation loss\n",
        "    # plt.plot(loss_history, label='Training Loss')\n",
        "    # plt.plot(val_loss_history, label='Validation Loss')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Loss')\n",
        "    # plt.title('Training and Validation Loss over Epochs')\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "    return theta\n"
      ],
      "metadata": {
        "id": "yjMO4BhHxOxg"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MOdel **"
      ],
      "metadata": {
        "id": "Bjd6KTHF2Pr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_training(lr,beta,beta1,beta2,epsilon,x_train,y_train,x_valid,y_valid,optimizer,batch_size,num_layers,hidden_size,active_fun_hidden, active_fun_output, weight_ini_method, epochs):\n",
        "  theta=initial_weight(num_layers,hidden_size,weight_ini_method)\n",
        "  if optimizer=='sgd':\n",
        "    theta=sgd(lr,x_train,y_train,x_valid,y_valid,batch_size,num_layers,hidden_size,active_fun_hidden,active_fun_output,weight_ini_method,epochs)\n",
        "  elif optimizer=='momemtum':\n",
        "    theta=mgd(lr,beta,x_train,y_train,x_valid,y_valid,batch_size,num_layers,hidden_size,active_fun_hidden,active_fun_output,weight_ini_method,epochs)\n",
        "  elif optimizer=='nag':\n",
        "    theta=nesterov_gd(lr,beta,x_train,y_train,x_valid,y_valid,batch_size,num_layers,hidden_size,active_fun_hidden,active_fun_output,weight_ini_method,epochs)\n",
        "  elif optimizer=='RMSprop':\n",
        "    theta=rmsprop(lr, beta, epsilon, x_train, y_train, x_valid, y_valid, batch_size, num_layers, hidden_size, active_fun_hidden, active_fun_output, weight_ini_method, epochs)\n",
        "  elif optimizer=='adam':\n",
        "    theta=adam(lr,beta1,beta2,epsilon, x_train, y_train, x_valid, y_valid, batch_size, num_layers, hidden_size, active_fun_hidden, active_fun_output, weight_ini_method, epochs)\n"
      ],
      "metadata": {
        "id": "Gnun3pTU2O6-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}